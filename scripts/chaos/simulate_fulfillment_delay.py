#!/usr/bin/env python3
"""Chaos scenario: hold fulfillment artifacts in MinIO to mimic backlog.

The script seeds synthetic shipment labels into a MinIO bucket under a
scenario-specific prefix. This represents labels/paperwork that never reach
the downstream workflow, helping operators validate alerts such as
``fulfillment_lag_minutes`` and run dashboard drills. A revert mode cleans up
all objects generated by the scenario.
"""

from __future__ import annotations

import argparse
import json
import os
import shlex
import subprocess
import sys
import time
from dataclasses import dataclass
from datetime import datetime, timedelta, timezone
from io import BytesIO
from typing import Any, Dict, List, Mapping, MutableMapping, Sequence
from urllib.parse import urlparse

import httpx
from minio import Minio
from minio.error import S3Error


@dataclass(slots=True)
class CommandResult:
    command: List[str]
    returncode: int
    stdout: str
    stderr: str
    duration_seconds: float


class ChaosError(RuntimeError):
    def __init__(self, message: str, *, context: Mapping[str, Any] | None = None) -> None:
        super().__init__(message)
        self.context: Dict[str, Any] = dict(context or {})


def _env_default(name: str, default: str) -> str:
    value = os.getenv(name)
    return value if value else default


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Hold fulfillment artifacts in MinIO to simulate backlog")
    parser.add_argument(
        "--minio-endpoint",
        default=_env_default("FULFILLMENT_DELAY_MINIO_ENDPOINT", "http://127.0.0.1:9000"),
        help="MinIO endpoint (default: %(default)s or FULFILLMENT_DELAY_MINIO_ENDPOINT)",
    )
    parser.add_argument(
        "--minio-access-key",
        default=_env_default("FULFILLMENT_DELAY_MINIO_ACCESS_KEY", "admin"),
        help="MinIO access key (default: %(default)s or FULFILLMENT_DELAY_MINIO_ACCESS_KEY)",
    )
    parser.add_argument(
        "--minio-secret-key",
        default=_env_default("FULFILLMENT_DELAY_MINIO_SECRET_KEY", "admin12345"),
        help="MinIO secret key (default: %(default)s or FULFILLMENT_DELAY_MINIO_SECRET_KEY)",
    )
    parser.add_argument(
        "--minio-region",
        default=_env_default("FULFILLMENT_DELAY_MINIO_REGION", "us-east-1"),
        help="MinIO region (default: %(default)s or FULFILLMENT_DELAY_MINIO_REGION)",
    )
    parser.add_argument(
        "--bucket",
        default=_env_default("FULFILLMENT_DELAY_BUCKET", "fulfillment-artifacts"),
        help="Bucket to store backlog artifacts (default: %(default)s or FULFILLMENT_DELAY_BUCKET)",
    )
    parser.add_argument(
        "--prefix",
        default=_env_default("FULFILLMENT_DELAY_PREFIX", "chaos/fulfillment-delay/"),
        help="Key prefix for synthetic artifacts (default: %(default)s or FULFILLMENT_DELAY_PREFIX)",
    )
    parser.add_argument(
        "--count",
        type=int,
        default=int(_env_default("FULFILLMENT_DELAY_COUNT", "25")),
        help="Number of artifacts to create (default: %(default)s or FULFILLMENT_DELAY_COUNT)",
    )
    parser.add_argument(
        "--hold-minutes",
        type=int,
        default=int(_env_default("FULFILLMENT_DELAY_HOLD_MINUTES", "30")),
        help="Minutes to mark objects as held (metadata only, default: %(default)s or FULFILLMENT_DELAY_HOLD_MINUTES)",
    )
    parser.add_argument(
        "--object-size-kb",
        type=int,
        default=int(_env_default("FULFILLMENT_DELAY_OBJECT_SIZE_KB", "128")),
        help="Approximate size per artifact in KiB (default: %(default)s or FULFILLMENT_DELAY_OBJECT_SIZE_KB)",
    )
    parser.add_argument(
        "--compose-cmd",
        default=_env_default("FULFILLMENT_DELAY_COMPOSE_CMD", "docker compose"),
        help="Base docker compose command (default: %(default)s or FULFILLMENT_DELAY_COMPOSE_CMD)",
    )
    parser.add_argument(
        "--compose-file",
        dest="compose_files",
        action="append",
        help="Additional docker compose files to include (-f). Can be passed multiple times.",
    )
    parser.add_argument(
        "--minio-service",
        default=_env_default("FULFILLMENT_DELAY_MINIO_SERVICE", "minio"),
        help="Docker compose service name for MinIO (default: %(default)s or FULFILLMENT_DELAY_MINIO_SERVICE)",
    )
    parser.add_argument(
        "--support-base-url",
        default=_env_default("FULFILLMENT_DELAY_SUPPORT_BASE_URL", "http://127.0.0.1:8109"),
        help="Support service base URL for optional timeline probe (default: %(default)s or FULFILLMENT_DELAY_SUPPORT_BASE_URL)",
    )
    parser.add_argument(
        "--support-ticket-id",
        help="If provided, fetch timeline for this ticket to observe downstream impact",
    )
    parser.add_argument(
        "--support-timeout",
        type=float,
        default=float(_env_default("FULFILLMENT_DELAY_SUPPORT_TIMEOUT", "5")),
        help="HTTP timeout (seconds) for support service probe (default: %(default)s or FULFILLMENT_DELAY_SUPPORT_TIMEOUT)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Print intended actions without modifying MinIO",
    )
    parser.add_argument(
        "--revert",
        action="store_true",
        help="Remove backlog artifacts created by this scenario instead of adding new ones",
    )

    args = parser.parse_args()

    if not args.revert and args.count <= 0:
        parser.error("--count must be positive unless --revert is used")
    if args.object_size_kb <= 0:
        parser.error("--object-size-kb must be positive")
    if args.hold_minutes <= 0:
        parser.error("--hold-minutes must be positive")

    compose_cmd = shlex.split(args.compose_cmd)
    if not compose_cmd:
        parser.error("--compose-cmd resolved to an empty command")

    compose_files_env = os.getenv("FULFILLMENT_DELAY_COMPOSE_FILES", "")
    compose_files: List[str] = []
    if args.compose_files:
        compose_files.extend(args.compose_files)
    elif compose_files_env:
        compose_files.extend([value for value in compose_files_env.split(":") if value])
    else:
        compose_files.append("docker-compose.yml")

    args.compose_cmd = compose_cmd
    args.compose_files = compose_files
    return args


def compose_command(args: argparse.Namespace, extra_args: Sequence[str], *, check: bool = True) -> CommandResult:
    command = list(args.compose_cmd)
    for compose_file in args.compose_files:
        command.extend(["-f", compose_file])
    command.extend(extra_args)
    start = time.perf_counter()
    proc = subprocess.run(command, capture_output=True, text=True)
    duration = time.perf_counter() - start
    result = CommandResult(
        command=command,
        returncode=proc.returncode,
        stdout=proc.stdout,
        stderr=proc.stderr,
        duration_seconds=duration,
    )
    if check and result.returncode != 0:
        raise ChaosError(
            "Docker compose command failed",
            context={
                "command": command,
                "returncode": result.returncode,
                "stderr": result.stderr.strip(),
            },
        )
    return result


def _parse_endpoint(endpoint: str) -> tuple[str, bool]:
    parsed = urlparse(endpoint)
    if parsed.scheme:
        host = parsed.netloc or parsed.path
        secure = parsed.scheme == "https"
    else:
        host = endpoint
        secure = not endpoint.startswith("http://")
    if not host:
        raise ChaosError("Invalid MinIO endpoint", context={"endpoint": endpoint})
    return host, secure


def _connect_minio(args: argparse.Namespace) -> Minio:
    host, secure = _parse_endpoint(args.minio_endpoint)
    return Minio(
        host,
        access_key=args.minio_access_key,
        secret_key=args.minio_secret_key,
        secure=secure,
        region=args.minio_region,
    )


def _ensure_bucket(client: Minio, bucket: str, *, region: str, dry_run: bool) -> Mapping[str, Any]:
    start = time.perf_counter()
    existed = False
    created = False
    try:
        existed = client.bucket_exists(bucket)
        if not existed and not dry_run:
            client.make_bucket(bucket, location=region)
            created = True
    except S3Error as exc:
        raise ChaosError("Failed to ensure bucket", context={"bucket": bucket, "error": str(exc)}) from exc
    duration = time.perf_counter() - start
    return {
        "bucket": bucket,
        "existed": existed,
        "created": created,
        "durationSeconds": round(duration, 3),
    }


def _normalize_prefix(prefix: str) -> str:
    if not prefix:
        return ""
    normalized = prefix.lstrip("/")
    if normalized and not normalized.endswith("/"):
        normalized += "/"
    return normalized


def _generate_artifact_content(order_id: int, artifact_id: str, hold_until: datetime, target_size_kb: int) -> bytes:
    header = (
        f"Synthetic fulfillment label\n"
        f"Order: {order_id}\n"
        f"Artifact: {artifact_id}\n"
        f"Hold until: {hold_until.isoformat()}\n"
    ).encode("utf-8")
    target_size = max(target_size_kb * 1024, len(header))
    padding = target_size - len(header)
    if padding > 0:
        filler = ("#" * padding).encode("utf-8")
    else:
        filler = b""
    return header + filler


def _generate_order_id() -> int:
    return 1_000_000 + int.from_bytes(os.urandom(3), "big")


def _generate_artifact_id() -> str:
    return os.urandom(6).hex()


def _create_backlog(
    client: Minio,
    *,
    bucket: str,
    prefix: str,
    count: int,
    hold_until: datetime,
    size_kb: int,
    dry_run: bool,
) -> Mapping[str, Any]:
    created: list[Mapping[str, Any]] = []
    total_bytes = 0
    start = time.perf_counter()

    for _ in range(count):
        order_id = _generate_order_id()
        artifact_id = _generate_artifact_id()
        object_name = f"{prefix}{order_id}-{artifact_id}.pdf"
        content = _generate_artifact_content(order_id, artifact_id, hold_until, size_kb)
        metadata = {
            "hold-until": hold_until.isoformat(),
            "chaos-scenario": "fulfillment-delay",
            "order-id": str(order_id),
            "artifact-id": artifact_id,
        }
        if not dry_run:
            try:
                data = BytesIO(content)
                client.put_object(
                    bucket,
                    object_name,
                    data,
                    length=len(content),
                    content_type="application/pdf",
                    metadata=metadata,
                )
            except S3Error as exc:
                raise ChaosError(
                    "Failed to upload backlog artifact",
                    context={"object": object_name, "error": str(exc)},
                ) from exc
        created.append(
            {
                "key": object_name,
                "orderId": order_id,
                "artifactId": artifact_id,
                "sizeBytes": len(content),
            }
        )
        total_bytes += len(content)

    duration = time.perf_counter() - start
    backlog_mb = round(total_bytes / (1024 * 1024), 3)
    preview = created[:5]
    return {
        "objectsCreated": 0 if dry_run else len(created),
        "objectsPlanned": len(created),
        "sample": preview,
        "totalBytes": total_bytes,
        "approxMiB": backlog_mb,
        "durationSeconds": round(duration, 3),
    }


def _list_backlog(client: Minio, *, bucket: str, prefix: str) -> list[Mapping[str, Any]]:
    objects: list[Mapping[str, Any]] = []
    try:
        for obj in client.list_objects(bucket, prefix=prefix, recursive=True):
            if obj.is_dir:
                continue
            objects.append({"key": obj.object_name, "sizeBytes": obj.size})
    except S3Error as exc:
        raise ChaosError("Failed to list backlog objects", context={"error": str(exc)}) from exc
    return objects


def _remove_backlog(
    client: Minio,
    *,
    bucket: str,
    prefix: str,
    dry_run: bool,
) -> Mapping[str, Any]:
    objects = _list_backlog(client, bucket=bucket, prefix=prefix)
    if not objects:
        return {
            "removed": 0,
            "candidates": 0,
            "sample": [],
            "totalBytes": 0,
            "approxMiB": 0.0,
        }

    if dry_run:
        total_bytes = sum(obj["sizeBytes"] for obj in objects)
        return {
            "removed": 0,
            "candidates": len(objects),
            "sample": objects[:5],
            "totalBytes": total_bytes,
            "approxMiB": round(total_bytes / (1024 * 1024), 3),
        }

    start = time.perf_counter()
    removed = 0
    total_bytes = 0
    for obj in objects:
        try:
            client.remove_object(bucket, obj["key"])
        except S3Error as exc:
            raise ChaosError("Failed to delete backlog object", context={"object": obj["key"], "error": str(exc)}) from exc
        removed += 1
        total_bytes += int(obj.get("sizeBytes", 0))
    duration = time.perf_counter() - start
    return {
        "removed": removed,
        "candidates": len(objects),
        "sample": objects[:5],
        "totalBytes": total_bytes,
        "approxMiB": round(total_bytes / (1024 * 1024), 3),
        "durationSeconds": round(duration, 3),
    }


def _support_probe(
    base_url: str,
    ticket_id: str | None,
    *,
    timeout: float,
) -> Mapping[str, Any] | None:
    if not ticket_id:
        return None

    url = base_url.rstrip("/") + f"/support/cases/{ticket_id}"
    params = {"includeTimeline": "true"}
    start = time.perf_counter()
    try:
        response = httpx.get(url, params=params, timeout=timeout)
        elapsed = time.perf_counter() - start
        response.raise_for_status()
        body = response.json()
    except (httpx.HTTPError, json.JSONDecodeError) as exc:  # noqa: PERF203
        return {
            "status": "error",
            "message": str(exc),
            "elapsedSeconds": round(time.perf_counter() - start, 3),
        }

    timeline = body.get("timeline", []) if isinstance(body, MutableMapping) else []
    latest_fulfillment: Mapping[str, Any] | None = None
    for entry in timeline:
        if not isinstance(entry, Mapping):
            continue
        if entry.get("source") == "fulfillment-service":
            if latest_fulfillment is None:
                latest_fulfillment = entry
            else:
                current_time = entry.get("timestamp") or entry.get("createdAt")
                stored_time = latest_fulfillment.get("timestamp") or latest_fulfillment.get("createdAt")
                if current_time and stored_time and current_time > stored_time:
                    latest_fulfillment = entry
    return {
        "status": "ok",
        "httpStatus": 200,
        "elapsedSeconds": round(elapsed, 3),
        "timelineEvents": len(timeline),
        "latestFulfillmentEvent": latest_fulfillment,
    }


def _compose_status(args: argparse.Namespace) -> Mapping[str, Any]:
    info: Dict[str, Any] = {}
    try:
        result_all = compose_command(args, ["ps", "--services"], check=False)
    except ChaosError as exc:
        return {"status": "error", "message": str(exc), "context": exc.context}

    services = [line.strip() for line in result_all.stdout.splitlines() if line.strip()]
    info["servicesRunning"] = services
    info["minioRunning"] = args.minio_service in services

    try:
        result_minio = compose_command(args, ["ps", args.minio_service], check=False)
        info["minioPs"] = result_minio.stdout.strip()
    except ChaosError as exc:
        info["minioPs"] = f"error: {exc}"
        info["minioContext"] = exc.context
    return info


def run(args: argparse.Namespace) -> Mapping[str, Any]:
    overall_start = time.perf_counter()
    events: list[Mapping[str, Any]] = []

    compose_info = _compose_status(args)
    events.append({"step": "compose-status", "details": compose_info})

    client = _connect_minio(args)
    bucket = args.bucket
    prefix = _normalize_prefix(args.prefix)

    bucket_result = _ensure_bucket(client, bucket, region=args.minio_region, dry_run=args.dry_run)
    events.append({"step": "ensure-bucket", "details": bucket_result})

    if args.revert:
        removal = _remove_backlog(client, bucket=bucket, prefix=prefix, dry_run=args.dry_run)
        events.append({"step": "remove-backlog", "details": removal})
        support = _support_probe(args.support_base_url, args.support_ticket_id, timeout=args.support_timeout)
        if support:
            events.append({"step": "support-probe", "details": support})
        return {
            "status": "ok",
            "mode": "revert",
            "bucket": bucket,
            "prefix": prefix,
            "dryRun": args.dry_run,
            "results": {
                "compose": compose_info,
                "bucket": bucket_result,
                "removal": removal,
                "support": support,
            },
            "events": events,
            "durationSeconds": round(time.perf_counter() - overall_start, 3),
        }

    hold_until = datetime.now(timezone.utc) + timedelta(minutes=args.hold_minutes)
    backlog = _create_backlog(
        client,
        bucket=bucket,
        prefix=prefix,
        count=args.count,
        hold_until=hold_until,
        size_kb=args.object_size_kb,
        dry_run=args.dry_run,
    )
    events.append({"step": "create-backlog", "details": backlog})

    support = _support_probe(args.support_base_url, args.support_ticket_id, timeout=args.support_timeout)
    if support:
        events.append({"step": "support-probe", "details": support})

    return {
        "status": "ok",
        "mode": "inject",
        "bucket": bucket,
        "prefix": prefix,
        "dryRun": args.dry_run,
        "holdUntil": hold_until.isoformat(),
        "countRequested": args.count,
        "objectSizeKiB": args.object_size_kb,
        "results": {
            "compose": compose_info,
            "bucket": bucket_result,
            "backlog": backlog,
            "support": support,
        },
        "events": events,
        "durationSeconds": round(time.perf_counter() - overall_start, 3),
    }


def main() -> int:
    args = parse_args()
    try:
        result = run(args)
    except ChaosError as exc:
        payload = {"status": "error", "message": str(exc), "context": exc.context}
        json.dump(payload, sys.stdout, indent=2)
        sys.stdout.write("\n")
        return 2
    except Exception as exc:  # noqa: BLE001
        payload = {"status": "error", "message": str(exc)}
        json.dump(payload, sys.stdout, indent=2)
        sys.stdout.write("\n")
        return 3

    json.dump(result, sys.stdout, indent=2)
    sys.stdout.write("\n")
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
